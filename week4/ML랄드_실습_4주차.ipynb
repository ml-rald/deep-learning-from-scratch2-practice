{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPKpqd8hyD+Gj9vLjToyIt9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# === MeCab 엔진 및 파이썬 바인딩 완벽 설치 (권장) ===\n","# Colab 런타임이 재시작될 때마다 이 셀을 가장 먼저 실행해주세요.\n","\n","import sys # sys 모듈을 미리 임포트합니다.\n","\n","print(\"단계 1/5: konlpy 설치 및 환경 업데이트...\")\n","print(\"----------------------------------------------------------------\")\n","!pip install konlpy  # konlpy 설치 (JPype1 포함)\n","!apt-get update      # 패키지 목록 업데이트\n","!apt-get install curl -y # curl 설치\n","print(\"----------------------------------------------------------------\")\n","\n","print(\"\\n단계 2/5: MeCab 엔진 및 한국어 사전 설치 (konlpy 공식 스크립트 사용)...\")\n","print(\"----------------------------------------------------------------\")\n","# konlpy 공식 github에서 제공하는 MeCab 엔진 및 사전 설치 스크립트 실행\n","# 이 스크립트가 MeCab 본체 (C++ 라이브러리)와 mecab-ko-dic 사전을 /usr/local/ 경로에 설치합니다.\n","!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n","print(\"----------------------------------------------------------------\")\n","\n","print(\"\\n단계 3/5: 기존 python-mecab-ko 제거 (깨끗한 재설치를 위해)...\")\n","print(\"----------------------------------------------------------------\")\n","# 이전 \"Requirement already satisfied: python-mecab-ko\" 메시지가 나왔지만,\n","# import가 안 되는 문제를 해결하기 위해 강제로 제거 후 재설치합니다.\n","!pip uninstall -y python-mecab-ko\n","print(\"----------------------------------------------------------------\")\n","\n","print(\"\\n단계 4/5: MeCab 파이썬 바인딩 재설치 (가장 안정적인 'mecab-python3' 사용)...\")\n","print(\"----------------------------------------------------------------\")\n","# 이전 \"subprocess-exited-with-error\" 오류를 우회하고,\n","# 'import MeCab' 문제를 해결하기 위해 가장 일반적인 바인딩을 명시적으로 설치합니다.\n","!pip install mecab-python3\n","print(\"----------------------------------------------------------------\")\n","\n","print(\"\\n단계 5/5: MeCab 설치 최종 진단 및 Tagger 초기화 테스트...\")\n","print(\"----------------------------------------------------------------\")\n","# MeCab 실행 파일 확인\n","!which mecab\n","# MeCab 버전 및 사전 경로 확인\n","!mecab -D # MeCab 엔진 및 사전 경로 확인\n","\n","# 파이썬 내에서 MeCab 모듈 임포트 및 Tagger 초기화 테스트\n","print(\"\\n파이썬 내 MeCab 모듈 임포트 및 Tagger 테스트:\")\n","try:\n","    import MeCab # 이제 이 임포트가 성공해야 합니다.\n","    print('MeCab 파이썬 모듈 임포트 성공!')\n","    MECAB_DIC_PATH = '/usr/local/lib/mecab/dic/mecab-ko-dic/' # MeCab 사전 경로\n","    # MeCab.Tagger 초기화 시 mecabrc 경로 지정(-r 옵션)을 제거하여 MeCab이 자동으로 찾도록 합니다.\n","    tagger = MeCab.Tagger(f'-d {MECAB_DIC_PATH}')\n","    print('MeCab Tagger 객체 성공적으로 생성됨.')\n","except ImportError as e:\n","    print(f'**치명적 오류**: MeCab 파이썬 모듈 임포트 실패! - {e}')\n","    print('MeCab 설치에 문제가 있습니다. Colab 런타임을 재시작하고 다시 시도하거나, 다른 Colab 환경을 사용해보세요.')\n","    sys.exit(1) # 오류 발생 시 스크립트 종료\n","except RuntimeError as e:\n","    print(f'**치명적 오류**: MeCab Tagger 객체 생성 실패! - {e}')\n","    print('MeCab Tagger 초기화에 문제가 있습니다. 사전 경로를 확인하거나 MeCab 설치를 재확인하세요.')\n","    sys.exit(1) # 오류 발생 시 스크립트 종료\n","except Exception as e:\n","    print(f'**치명적 오류**: 예상치 못한 오류 발생: {e}')\n","    sys.exit(1) # 오류 발생 시 스크립트 종료\n","print(\"----------------------------------------------------------------\")\n","print(\"\\n모든 설치 및 진단 단계 완료. 위의 파이썬 테스트 결과에 따라 다음 진행 여부가 결정됩니다.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PgDKiTKRxwW-","executionInfo":{"status":"ok","timestamp":1751120664233,"user_tz":-540,"elapsed":25337,"user":{"displayName":"우정아","userId":"00812420541904868241"}},"outputId":"0927cf11-07d9-4204-c15a-69d675d4e73a"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["단계 1/5: konlpy 설치 및 환경 업데이트...\n","----------------------------------------------------------------\n","Requirement already satisfied: konlpy in /usr/local/lib/python3.11/dist-packages (0.6.0)\n","Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from konlpy) (1.5.2)\n","Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.11/dist-packages (from konlpy) (5.4.0)\n","Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.11/dist-packages (from konlpy) (2.0.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from JPype1>=0.7.0->konlpy) (24.2)\n","Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n","Hit:2 http://security.ubuntu.com/ubuntu jammy-security InRelease\n","Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n","Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n","Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n","Hit:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n","Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n","Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n","Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n","Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n","Reading package lists... Done\n","W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","curl is already the newest version (7.81.0-1ubuntu1.20).\n","0 upgraded, 0 newly installed, 0 to remove and 36 not upgraded.\n","----------------------------------------------------------------\n","\n","단계 2/5: MeCab 엔진 및 한국어 사전 설치 (konlpy 공식 스크립트 사용)...\n","----------------------------------------------------------------\n","mecab-ko is already installed\n","mecab-ko-dic is already installed\n","mecab-python is already installed\n","Done.\n","----------------------------------------------------------------\n","\n","단계 3/5: 기존 python-mecab-ko 제거 (깨끗한 재설치를 위해)...\n","----------------------------------------------------------------\n","\u001b[33mWARNING: Skipping python-mecab-ko as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0m----------------------------------------------------------------\n","\n","단계 4/5: MeCab 파이썬 바인딩 재설치 (가장 안정적인 'mecab-python3' 사용)...\n","----------------------------------------------------------------\n","Requirement already satisfied: mecab-python3 in /usr/local/lib/python3.11/dist-packages (1.0.10)\n","----------------------------------------------------------------\n","\n","단계 5/5: MeCab 설치 최종 진단 및 Tagger 초기화 테스트...\n","----------------------------------------------------------------\n","/usr/local/bin/mecab\n","filename:\t/usr/local/lib/mecab/dic/mecab-ko-dic/sys.dic\n","version:\t102\n","charset:\tUTF-8\n","type:\t0\n","size:\t816283\n","left size:\t3822\n","right size:\t2693\n","\n","\n","파이썬 내 MeCab 모듈 임포트 및 Tagger 테스트:\n","MeCab 파이썬 모듈 임포트 성공!\n","MeCab Tagger 객체 성공적으로 생성됨.\n","----------------------------------------------------------------\n","\n","모든 설치 및 진단 단계 완료. 위의 파이썬 테스트 결과에 따라 다음 진행 여부가 결정됩니다.\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from collections import Counter\n","import MeCab\n","import sys\n","import json\n","from typing import List, Tuple, Dict, Any\n","import numpy as np\n","import time # 시간 측정을 위한 time 모듈 임포트\n","\n","# --- 1. 환경 설정 및 유틸리티 함수 ---\n","\n","# initialize_mecab_tagger 함수 수정: mecabrc_path 인자 사용 방식 변경 (이전 수정과 동일)\n","def initialize_mecab_tagger(dic_path: str) -> MeCab.Tagger:\n","    \"\"\"\n","    MeCab Tagger를 초기화하고 반환합니다.\n","    Args:\n","        dic_path (str): MeCab 한국어 사전의 루트 디렉토리 경로.\n","    Returns:\n","        MeCab.Tagger: 초기화된 MeCab Tagger 객체.\n","    Raises:\n","        RuntimeError: Tagger 초기화에 실패할 경우 발생.\n","    \"\"\"\n","    try:\n","        tagger = MeCab.Tagger(f'-d {dic_path}')\n","        print(f\"MeCab Tagger가 성공적으로 초기화되었습니다.\")\n","        print(f\"  - 사전 경로: {dic_path}\")\n","        return tagger\n","    except RuntimeError as e:\n","        print(f\"MeCab Tagger 초기화 오류: {e}\")\n","        print(f\"  - 시도된 사전 경로: {dic_path}\")\n","        print(\"MeCab Tagger 초기화에 실패했습니다. MeCab 설치 및 경로 설정을 다시 확인해주세요.\")\n","        sys.exit(1) # 오류 발생 시 프로그램 종료\n","\n","\n","def tokenize_sentences(tagger: MeCab.Tagger, sentences: List[str]) -> List[List[str]]:\n","    \"\"\"\n","    문장 리스트를 형태소 분석하여 토큰화된 문장 리스트를 반환합니다.\n","    \"\"\"\n","    tokenized_sentences = []\n","    start_time = time.time()\n","    for i, sentence in enumerate(sentences):\n","        if i > 0 and i % 5000 == 0: # 5천개 문장마다 진행 상황 출력\n","            print(f\"  - {i}/{len(sentences)} 문장 토큰화 완료 ({time.time() - start_time:.2f} 초)\")\n","        if not sentence or not isinstance(sentence, str):\n","            continue\n","        try:\n","            parsed_lines = tagger.parse(sentence).strip().split('\\n')\n","        except Exception as e:\n","            # print(f\"  경고: 문장 토큰화 중 오류 발생: {sentence[:50]}... - {e}\") # 디버깅용\n","            continue\n","\n","        sentence_tokens = []\n","        for line in parsed_lines:\n","            if '\\t' in line:\n","                word = line.split('\\t')[0]\n","                if '+' in word and not word.startswith('+'):\n","                    word = word.split('+')[0]\n","                sentence_tokens.append(word)\n","        if sentence_tokens:\n","            tokenized_sentences.append(sentence_tokens)\n","    print(f\"  - 모든 문장 토큰화 완료. 총 {len(tokenized_sentences)}개 유효 문장 ({time.time() - start_time:.2f} 초 소요)\")\n","    return tokenized_sentences\n","\n","def build_vocab(tokenized_sentences: List[List[str]], min_count: int = 5) -> Tuple[Dict[str, int], Dict[int, str], int]:\n","    \"\"\"\n","    토큰화된 문장들로부터 단어 사전(vocab)을 구축합니다.\n","    \"\"\"\n","    start_time = time.time()\n","    all_words = [word for sentence in tokenized_sentences for word in sentence]\n","    word_counts = Counter(all_words)\n","\n","    filtered_word_list = [word for word, count in word_counts.items() if count >= min_count]\n","    word_list = sorted(filtered_word_list)\n","\n","    word_to_idx = {word: i for i, word in enumerate(word_list)}\n","    idx_to_word = {i: word for word, i in word_to_idx.items()}\n","    vocab_size = len(word_to_idx)\n","\n","    print(f\"단어 사전 크기: {vocab_size} (최소 빈도 {min_count} 이상) ({time.time() - start_time:.2f} 초 소요)\")\n","    return word_to_idx, idx_to_word, vocab_size\n","\n","def create_cbow_dataset(\n","    tokenized_sentences: List[List[str]],\n","    word_to_idx: Dict[str, int],\n","    window_size: int = 1\n",") -> Tuple[torch.Tensor, torch.Tensor]:\n","    \"\"\"\n","    CBOW 모델 학습을 위한 컨텍스트-타겟 쌍 데이터셋을 생성합니다.\n","    \"\"\"\n","    context_indices: List[List[int]] = []\n","    target_indices: List[int] = []\n","\n","    start_time = time.time()\n","    for i, tokens in enumerate(tokenized_sentences):\n","        if i > 0 and i % 5000 == 0: # 5천개 문장마다 진행 상황 출력\n","            print(f\"  - {i}/{len(tokenized_sentences)} 문장 CBOW 데이터 생성 진행 중 ({time.time() - start_time:.2f} 초)\")\n","\n","        if len(tokens) <= 2 * window_size:\n","            continue\n","\n","        for j in range(window_size, len(tokens) - window_size):\n","            target_word = tokens[j]\n","\n","            context_words = []\n","            for k in range(1, window_size + 1):\n","                context_words.append(tokens[j - k])\n","                context_words.append(tokens[j + k])\n","\n","            if target_word in word_to_idx and all(word in word_to_idx for word in context_words):\n","                context_indices.append([word_to_idx[word] for word in context_words])\n","                target_indices.append(word_to_idx[target_word])\n","\n","    if not context_indices:\n","        print(\"경고: CBOW 학습 데이터가 생성되지 않았습니다. 문장 길이, 윈도우 크기, 또는 단어 사전을 확인하세요.\")\n","        return torch.tensor([]), torch.tensor([])\n","\n","    contexts_tensor = torch.tensor(context_indices, dtype=torch.long)\n","    targets_tensor = torch.tensor(target_indices, dtype=torch.long)\n","\n","    print(f\"CBOW 컨텍스트 데이터 형태: {contexts_tensor.shape}\")\n","    print(f\"CBOW 타겟 데이터 형태: {targets_tensor.shape}\")\n","    print(f\"  - CBOW 학습 데이터 생성 완료 ({time.time() - start_time:.2f} 초 소요)\")\n","    return contexts_tensor, targets_tensor\n","\n","# --- 2. CBOW 모델 정의 ---\n","\n","class CBOW(nn.Module):\n","    \"\"\"\n","    Continuous Bag-of-Words (CBOW) 모델 구현.\n","    \"\"\"\n","    def __init__(self, vocab_size: int, embedding_dim: int):\n","        super(CBOW, self).__init__()\n","        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n","        self.linear = nn.Linear(embedding_dim, vocab_size)\n","\n","    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n","        embeds = self.embeddings(inputs)\n","        mean_embeds = embeds.mean(dim=1)\n","        output = self.linear(mean_embeds)\n","        return output\n","\n","# --- 3. 학습 함수 정의 ---\n","\n","def train_cbow_model(\n","    model: CBOW,\n","    contexts: torch.Tensor,\n","    targets: torch.Tensor,\n","    epochs: int = 100,\n","    learning_rate: float = 0.01,\n","    device: torch.device = torch.device('cpu')\n",") -> None:\n","    if contexts.numel() == 0:\n","        print(\"학습 데이터가 없어 모델 학습을 건너킵니다.\")\n","        return\n","\n","    model.to(device)\n","    contexts = contexts.to(device)\n","    targets = targets.to(device)\n","\n","    loss_fn = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    print(f\"\\nCBOW 모델 학습 시작 (장치: {device})...\")\n","    start_time = time.time()\n","    for epoch in range(epochs):\n","        model.train()\n","        optimizer.zero_grad()\n","\n","        output = model(contexts)\n","        loss = loss_fn(output, targets)\n","        loss.backward()\n","        optimizer.step()\n","\n","        if (epoch + 1) % 10 == 0:\n","            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f} ({time.time() - start_time:.2f} 초 경과)\")\n","    print(\"CBOW 모델 학습 완료.\")\n","\n","def get_word_embedding(model: CBOW, word: str, word_to_idx: Dict[str, int]) -> np.ndarray:\n","    \"\"\"\n","    특정 단어의 학습된 임베딩 벡터를 반환합니다.\n","    \"\"\"\n","    if word not in word_to_idx:\n","        raise ValueError(f\"'{word}'는 단어 사전에 없습니다.\")\n","\n","    word_id = word_to_idx[word]\n","    embedding = model.embeddings(torch.tensor([word_id], dtype=torch.long, device=model.embeddings.weight.device))\n","    return embedding.detach().cpu().numpy()\n","\n","# --- 4. JSON 파일 로드 함수 ---\n","\n","def load_sentences_from_json(file_path: str, max_sentences: int = -1) -> List[str]:\n","    \"\"\"\n","    'train_original.json' 파일의 특정 구조에 맞춰 문장 데이터를 로드합니다.\n","    \"\"\"\n","    sentences_list: List[str] = []\n","    start_time = time.time()\n","    try:\n","        with open(file_path, 'r', encoding='utf-8') as f:\n","            data = json.load(f)\n","\n","            if isinstance(data, dict):\n","                if \"documents\" in data and isinstance(data[\"documents\"], list):\n","                    for doc_item in data[\"documents\"]:\n","                        if max_sentences != -1 and len(sentences_list) >= max_sentences:\n","                            break\n","\n","                        if isinstance(doc_item, dict) and \"text\" in doc_item:\n","                            text_content = doc_item[\"text\"]\n","                            if isinstance(text_content, list):\n","                                for sentence_block in text_content:\n","                                    if isinstance(sentence_block, list):\n","                                        for sentence_obj in sentence_block:\n","                                            if max_sentences != -1 and len(sentences_list) >= max_sentences:\n","                                                break\n","\n","                                            if isinstance(sentence_obj, dict) and \"sentence\" in sentence_obj:\n","                                                if isinstance(sentence_obj[\"sentence\"], str):\n","                                                    sentences_list.append(sentence_obj[\"sentence\"])\n","                                        if max_sentences != -1 and len(sentences_list) >= max_sentences:\n","                                            break\n","                else:\n","                    print(\"오류: JSON 파일의 최상위 딕셔너리에서 'documents' 키를 찾을 수 없거나 해당 키의 값이 리스트가 아닙니다.\")\n","                    sys.exit(1)\n","            else:\n","                print(f\"오류: JSON 파일의 최상위 구조가 딕셔너리가 아닙니다. 지원되지 않는 형식입니다: {type(data)}\")\n","                sys.exit(1)\n","\n","    except FileNotFoundError:\n","        print(f\"오류: 파일을 찾을 수 없습니다 - {file_path}\")\n","        sys.exit(1)\n","    except json.JSONDecodeError as e:\n","        print(f\"오류: JSON 디코딩 실패 - {file_path}. 파일 내용이 유효한 JSON 형식이 아닐 수 있습니다: {e}\")\n","        sys.exit(1)\n","    except Exception as e:\n","        print(f\"파일 로드 중 알 수 없는 오류 발생: {e}\")\n","        sys.exit(1)\n","\n","    print(f\"총 {len(sentences_list)}개의 문장을 '{file_path}'에서 로드했습니다. ({time.time() - start_time:.2f} 초 소요)\")\n","    return sentences_list\n","\n","# --- 5. 메인 실행 블록 ---\n","\n","if __name__ == \"__main__\":\n","    # --- MeCab 설정 (코랩 환경에 맞춰 미리 설정됨) ---\n","    MECAB_DIC_PATH = \"/usr/local/lib/mecab/dic/mecab-ko-dic/\" # 코랩 MeCab 사전 경로\n","\n","    # --- JSON 파일 경로 및 데이터 로드 설정 (코랩 로컬 업로드용) ---\n","    JSON_FILE_PATH = \"/content/train_original.json\" # <-- 코랩 로컬 업로드 시 이 경로를 사용합니다!\n","\n","    # 테스트를 위해 문장 수를 획기적으로 줄였습니다.\n","    # 이 값을 50000으로 다시 늘리려면 이 테스트가 성공한 후에 시도하세요.\n","    MAX_SENTENCES_TO_LOAD = 5000 # <-- 일단 1000개 문장만 로드하여 테스트.\n","    MIN_WORD_COUNT = 5 # <-- 단어 사전에 포함할 단어의 최소 빈도.\n","\n","    # --- PyTorch 장치 설정 ---\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    print(f\"학습 장치: {device}\")\n","\n","    # 1. MeCab Tagger 초기화\n","    print(\"\\n[1단계: MeCab Tagger 초기화]\")\n","    start_time_step = time.time()\n","    mecab_tagger = initialize_mecab_tagger(MECAB_DIC_PATH)\n","    print(f\"1단계 완료. ({time.time() - start_time_step:.2f} 초 소요)\")\n","\n","    # 2. JSON 파일에서 문장 데이터 로드\n","    print(\"\\n[2단계: JSON 파일에서 문장 데이터 로드]\")\n","    start_time_step = time.time()\n","    raw_sentences = load_sentences_from_json(JSON_FILE_PATH, MAX_SENTENCES_TO_LOAD)\n","    print(f\"2단계 완료. ({time.time() - start_time_step:.2f} 초 소요)\")\n","\n","    # 3. 형태소 분석 및 토큰화\n","    print(\"\\n[3단계: 형태소 분석 및 토큰화]\")\n","    start_time_step = time.time()\n","    tokenized_data = tokenize_sentences(mecab_tagger, raw_sentences)\n","    if not tokenized_data:\n","        print(\"경고: 형태소 분석 결과 생성된 유효한 토큰 데이터가 없습니다. 원본 데이터 또는 MeCab 설정을 확인하세요.\")\n","        sys.exit(1)\n","    print(f\"\\n[토큰화 샘플 (총 {len(tokenized_data)}개 문장 중 처음 5개)]\")\n","    for i, s in enumerate(tokenized_data[:5]):\n","        print(f\"  {i+1}: {s}\")\n","    print(f\"3단계 완료. ({time.time() - start_time_step:.2f} 초 소요)\")\n","\n","\n","    # 4. 단어 사전 구축\n","    print(\"\\n[4단계: 단어 사전 구축]\")\n","    start_time_step = time.time()\n","    word_to_idx, idx_to_word, vocab_size = build_vocab(tokenized_data, min_count=MIN_WORD_COUNT)\n","    print(f\"4단계 완료. ({time.time() - start_time_step:.2f} 초 소요)\")\n","\n","    # 5. CBOW 학습 데이터 생성\n","    print(\"\\n[5단계: CBOW 학습 데이터 생성]\")\n","    start_time_step = time.time()\n","    WINDOW_SIZE = 1 # 컨텍스트 윈도우 크기\n","    contexts_tensor, targets_tensor = create_cbow_dataset(tokenized_data, word_to_idx, WINDOW_SIZE)\n","    print(f\"5단계 완료. ({time.time() - start_time_step:.2f} 초 소요)\")\n","\n","    # 6. CBOW 모델 학습\n","    print(\"\\n[6단계: CBOW 모델 학습]\")\n","    EMBEDDING_DIM = 10 # 임베딩 차원\n","    if vocab_size > 0 and contexts_tensor.numel() > 0: # 단어 사전과 학습 데이터가 있을 때만 모델 생성 및 학습\n","        cbow_model = CBOW(vocab_size, EMBEDDING_DIM)\n","        train_cbow_model(cbow_model, contexts_tensor, targets_tensor,\n","                         epochs=300, learning_rate=0.005, device=device) # 에포크와 학습률 조정\n","\n","        # 7. 특정 단어의 임베딩 확인 (예시 단어 - 법률 문서 관련)\n","        print(\"\\n[7단계: 테스트 단어 임베딩 확인]\")\n","        test_words = [\"법률\", \"판결\", \"계약\", \"소송\", \"원고\", \"피고\", \"재판\", \"사건\", \"권리\"]\n","        print(\"\\n[테스트 단어 임베딩 확인]\")\n","        for word in test_words:\n","            try:\n","                embedding_vector = get_word_embedding(cbow_model, word, word_to_idx)\n","                print(f\"'{word}'의 임베딩: {embedding_vector}\")\n","            except ValueError as e:\n","                print(f\"{e}\")\n","    else:\n","        print(\"\\n단어 사전이 비어있거나 학습 데이터가 없어 모델 학습을 진행할 수 없습니다.\")\n","\n","    print(\"\\n모든 코드 실행 완료!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7J494z1HuOqs","outputId":"4a7cfb9f-0fa5-443f-dc96-1425d10b9b21","executionInfo":{"status":"ok","timestamp":1751122839474,"user_tz":-540,"elapsed":1724200,"user":{"displayName":"우정아","userId":"00812420541904868241"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["학습 장치: cpu\n","\n","[1단계: MeCab Tagger 초기화]\n","MeCab Tagger가 성공적으로 초기화되었습니다.\n","  - 사전 경로: /usr/local/lib/mecab/dic/mecab-ko-dic/\n","1단계 완료. (0.02 초 소요)\n","\n","[2단계: JSON 파일에서 문장 데이터 로드]\n","총 5000개의 문장을 '/content/train_original.json'에서 로드했습니다. (1.70 초 소요)\n","2단계 완료. (1.81 초 소요)\n","\n","[3단계: 형태소 분석 및 토큰화]\n","  - 모든 문장 토큰화 완료. 총 5000개 유효 문장 (1.80 초 소요)\n","\n","[토큰화 샘플 (총 5000개 문장 중 처음 5개)]\n","  1: ['원고', '가', '소속', '회사', '의', '노동조합', '에서', '분규', '가', '발생', '하', '자', '노조', '활동', '을', '구실', '로', '정상', '적', '인', '근무', '를', '해태', '하', '고', ',']\n","  2: ['노조', '조합장', '이', '사임', '한', '경우', ',']\n","  3: ['노동조합', '규약', '에', '동', '조합장', '의', '직무', '를', '대행', '할', '자', '를', '규정', '해', '두', '고', '있', '음', '에', '도', '원고', '자신', '이', '주동', '하', '여', '노조', '자치', '수습', '대책', '위원회', '를', '구성', '하', '여', '그', '위원장', '으로', '피선', '되', '어', '근무', '시간', '중', '에', '도', '노조', '활동', '을', '벌여', '운수', '업체', '인', '소속', '회사', '의', '업무', '에', '지장', '을', '초래', '하', '고']\n","  4: ['종업원', '들', '에게', '도', '나쁜', '영향', '을', '끼쳐', '소속', '회사', '가', '취업규칙', '을', '위반', '하', '고']\n","  5: ['고의로', '회사', '업무', '능률', '을', '저해', '하', '였으며', '회사', '업무', '상', '의', '지휘명령', '에', '위반', '하', '였음을', '이유', '로', '원고', '를', '징계', '해고', '하', '였', '다면', ',']\n","3단계 완료. (1.80 초 소요)\n","\n","[4단계: 단어 사전 구축]\n","단어 사전 크기: 2328 (최소 빈도 5 이상) (0.05 초 소요)\n","4단계 완료. (0.05 초 소요)\n","\n","[5단계: CBOW 학습 데이터 생성]\n","CBOW 컨텍스트 데이터 형태: torch.Size([127962, 2])\n","CBOW 타겟 데이터 형태: torch.Size([127962])\n","  - CBOW 학습 데이터 생성 완료 (0.63 초 소요)\n","5단계 완료. (0.64 초 소요)\n","\n","[6단계: CBOW 모델 학습]\n","\n","CBOW 모델 학습 시작 (장치: cpu)...\n","Epoch 10/300, Loss: 7.6806 (55.15 초 경과)\n","Epoch 20/300, Loss: 7.4786 (109.23 초 경과)\n","Epoch 30/300, Loss: 7.2655 (166.07 초 경과)\n","Epoch 40/300, Loss: 7.0276 (220.64 초 경과)\n","Epoch 50/300, Loss: 6.7587 (275.46 초 경과)\n","Epoch 60/300, Loss: 6.4629 (329.89 초 경과)\n","Epoch 70/300, Loss: 6.1585 (385.05 초 경과)\n","Epoch 80/300, Loss: 5.8713 (443.36 초 경과)\n","Epoch 90/300, Loss: 5.6203 (498.12 초 경과)\n","Epoch 100/300, Loss: 5.4120 (553.31 초 경과)\n","Epoch 110/300, Loss: 5.2433 (624.95 초 경과)\n","Epoch 120/300, Loss: 5.1051 (680.12 초 경과)\n","Epoch 130/300, Loss: 4.9866 (736.26 초 경과)\n","Epoch 140/300, Loss: 4.8815 (791.89 초 경과)\n","Epoch 150/300, Loss: 4.7870 (847.04 초 경과)\n","Epoch 160/300, Loss: 4.7014 (903.57 초 경과)\n","Epoch 170/300, Loss: 4.6228 (960.38 초 경과)\n","Epoch 180/300, Loss: 4.5502 (1019.91 초 경과)\n","Epoch 190/300, Loss: 4.4825 (1076.19 초 경과)\n","Epoch 200/300, Loss: 4.4192 (1132.68 초 경과)\n","Epoch 210/300, Loss: 4.3598 (1206.55 초 경과)\n","Epoch 220/300, Loss: 4.3039 (1262.58 초 경과)\n","Epoch 230/300, Loss: 4.2512 (1318.62 초 경과)\n","Epoch 240/300, Loss: 4.2014 (1375.16 초 경과)\n","Epoch 250/300, Loss: 4.1544 (1431.33 초 경과)\n","Epoch 260/300, Loss: 4.1101 (1487.82 초 경과)\n","Epoch 270/300, Loss: 4.0683 (1544.37 초 경과)\n","Epoch 280/300, Loss: 4.0288 (1600.17 초 경과)\n","Epoch 290/300, Loss: 3.9915 (1656.27 초 경과)\n","Epoch 300/300, Loss: 3.9563 (1712.58 초 경과)\n","CBOW 모델 학습 완료.\n","\n","[7단계: 테스트 단어 임베딩 확인]\n","\n","[테스트 단어 임베딩 확인]\n","'법률'의 임베딩: [[-0.33366475  0.41876474  1.375025    0.7743893   1.6112163  -0.7689169\n","   0.42888913 -1.6947291   1.7308568   0.8283648 ]]\n","'판결'의 임베딩: [[ 0.0728576  -0.9978399  -0.15602177  0.46363074  0.6330304   0.13779487\n","   0.65360016 -0.21965386  2.533177    0.7102688 ]]\n","'계약'의 임베딩: [[-0.46267542 -0.44770253  1.5333     -0.10051663  1.8690636  -0.09168173\n","   1.5162728  -0.6015349   1.4288367   0.28780663]]\n","'소송'의 임베딩: [[-0.06089913 -0.12558597  0.12824765  0.8714724   0.10550471  0.13617353\n","   1.0521957  -0.81208974  1.1364356   1.0887852 ]]\n","'원고'의 임베딩: [[-0.480602    0.42391813 -0.6922654   2.452009   -0.27529386  0.4756596\n","   0.32130438 -0.8596183   2.3904088   0.72593457]]\n","'피고'의 임베딩: [[-0.6078956   0.40516412 -0.20454323  0.5739408   1.602866    0.8736481\n","   0.86178285 -2.0435433  -0.07297949  0.00920571]]\n","'재판'의 임베딩: [[ 0.77470416  1.2622832   0.12720269  1.1158212   0.4550745   0.98135984\n","   0.5999774  -1.1759222   2.2794087   1.5498129 ]]\n","'사건'의 임베딩: [[ 0.06549367 -1.0049136   0.14772269  0.43801868  0.6666808   1.4660169\n","   0.69400245 -1.4433235   1.2386528   0.9966236 ]]\n","'권리'의 임베딩: [[-0.78792465 -1.008265    1.0467888  -0.16853487  0.5948024  -0.1643272\n","   1.6211269  -2.0880551  -0.21992327 -0.06458192]]\n","\n","모든 코드 실행 완료!\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"UumLSk9exU46"},"execution_count":null,"outputs":[]}]}